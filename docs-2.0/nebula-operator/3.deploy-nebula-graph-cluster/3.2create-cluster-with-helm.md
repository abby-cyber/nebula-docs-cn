# 使用 Helm 部署{{nebula.name}}集群

!!! Compatibility "历史版本兼容性"

    1.x 版本的 NebulaGraph Operator 不兼容 3.x 以下版本的{{nebula.name}}。

## 前提条件

- [安装 NebulaGraph Operator](../2.deploy-nebula-operator.md)
- [已创建 StorageClass](https://kubernetes.io/docs/concepts/storage/storage-classes/)
{{ent.ent_begin}}
- [已安装 LM 并加载 License Key](3.0.deploy-lm.md)
{{ent.ent_end}}

## 创建{{nebula.name}}集群

1. 添加 NebulaGraph Operator Helm 仓库
   
  ```bash
  helm repo add nebula-operator https://vesoft-inc.github.io/nebula-operator/charts
  ```

2. 更新 Helm 仓库，拉取最新仓库资源。
   
  ```bash
  helm repo update
  ```

3. 为安装集群所需的配置参数设置环境变量。
   
  ```bash
  export NEBULA_CLUSTER_NAME=nebula         #{{nebula.name}}集群的名字。
  export NEBULA_CLUSTER_NAMESPACE=nebula    #{{nebula.name}}集群所处的命名空间的名字。
  export STORAGE_CLASS_NAME=fast-disks             #{{nebula.name}}集群的 StorageClass。
  ```

4. 为{{nebula.name}}集群创建命名空间（如已创建，略过此步）。

  ```bash
  kubectl create namespace "${NEBULA_CLUSTER_NAMESPACE}"
  ```

  {{ent.ent_begin}}
5. 创建 Secret，用于拉取私有仓库中{{nebula.name}}镜像。

  ```bash
  kubectl -n "${NEBULA_CLUSTER_NAMESPACE}" create secret docker-registry <image-pull-secret> \
  --docker-server=DOCKER_REGISTRY_SERVER \
  --docker-username=DOCKER_USER \
  --docker-password=DOCKER_PASSWORD
  ```

  - `<image-pull-secret>`：指定 Secret 的名称。
  - DOCKER_REGISTRY_SERVE：指定拉取镜像的私有仓库服务器地址，例如`reg.example-inc.com`。
  - DOCKER_USE：镜像仓库用户名。
  - DOCKER_PASSWORD：镜像仓库密码。

  {{ent.ent_end}}

6. 创建{{nebula.name}}集群。

  ```bash
  helm install "${NEBULA_CLUSTER_NAME}" nebula-operator/nebula-cluster \
      {{ ent.ent_begin }}
      # 配置指向 LM 访问地址和端口，默认端口为`9119`。必须配置此参数以获取 License 信息。
      --set nebula.metad.licenseManagerURL=`192.168.8.XXX:9119` \
      # 配置集群中各服务的镜像地址。
      --set nebula.graphd.image=<reg.example-inc.com/test/graphd-ent> \
      --set nebula.metad.image=<reg.example-inc.com/test/metad-ent> \
      --set nebula.storaged.image=<reg.example-inc.com/test/storaged-ent> \
      # 配置拉取私有仓库中镜像的 Secret。
      --set nebula.imagePullSecrets=<image-pull-secret> \
      {{ ent.ent_end }}
      --set nameOverride=${NEBULA_CLUSTER_NAME} \
      --set nebula.storageClassName="${STORAGE_CLASS_NAME}" \
      # 指定{{nebula.name}}集群的版本。
      --set nebula.version=v{{nebula.release}} \
      # 指定集群 chart 的版本，不指定则默认安装最新版本 chart。
      # 执行 helm search repo nebula-operator/nebula-cluster 命令可查看所有 chart 版本。
      --version={{operator.release}} \
      --namespace="${NEBULA_CLUSTER_NAMESPACE}" \      
  ```

    {{ ent.ent_begin }}

  {{nebula.name}} Operator 支持通过 Helm 创建带 Zone 的集群。有关如何在 Operator 中使用 Zone 的详细信息，参见[创建带 Zone 的集群](3.1create-cluster-with-kubectl.md)。

  创建带 Zone 的集群示例：

  ```bash
  helm install "${NEBULA_CLUSTER_NAME}" nebula-operator/nebula-cluster \
      # 配置指向 LM 访问地址和端口，默认端口为`9119`。必须配置此参数以获取 License 信息。
      --set nebula.metad.licenseManagerURL=`192.168.8.XXX:9119` \
      # 配置集群中各服务的镜像地址。
      --set nebula.graphd.image=<reg.example-inc.com/test/graphd-ent> \
      --set nebula.metad.image=<reg.example-inc.com/test/metad-ent> \
      --set nebula.storaged.image=<reg.example-inc.com/test/storaged-ent> \
      # 配置拉取私有仓库中镜像的 Secret。
      --set nebula.imagePullSecrets=<image-pull-secret> \
      --set nameOverride=${NEBULA_CLUSTER_NAME} \
      --set nebula.storageClassName="${STORAGE_CLASS_NAME}" \
      # 指定{{nebula.name}}集群的版本。
      --set nebula.version=v{{nebula.release}} \
      # 指定集群 chart 的版本，不指定则默认安装最新版本 chart。
      # 执行 helm search repo nebula-operator/nebula-cluster 命令可查看所有 chart 版本。
      --version={{operator.release}} \
      # 配置 Zone。
      # 配置 Zone 后，Zone 的信息不能修改。建议配置奇数个 Zone。
      --set nebula.metad.config.zone_list=<zone1,zone2,zone3> \
      --set nebula.graphd.config.prioritize_intra_zone_reading=true \
      --set nebula.graphd.config.stick_to_intra_zone_on_failure=false \
      # 设置均分 Storage Pod 到不同 Zone。
      --set nebula.topologySpreadConstraints[0].topologyKey=topology.kubernetes.io/zone \
      --set nebula.topologySpreadConstraints[0].whenUnsatisfiable=DoNotSchedule \
      --namespace="${NEBULA_CLUSTER_NAMESPACE}" \      
  ```  

  !!! caution

        创建集群后，导入数据前，确保存储 Pod 均匀分布在各个 Zone 中以提高集群的韧性。运行`SHOW ZONES`命令来检查存储 Pod 是否均匀分布在各个 Zone 中。有关 Zone 相关命令，请参阅[管理 Zone](../../4.deployment-and-installation/5.zone.md)。

    {{ ent.ent_end }}
  
  执行`helm show values nebula-operator/nebula-cluster`命令，或者单击 [nebula-cluster/values.yaml
](https://github.com/vesoft-inc/nebula-operator/blob/{{operator.branch}}/charts/nebula-cluster/values.yaml) 可查看{{nebula.name}}集群的所有配置参数。

  单击 [Chart parameters](https://github.com/vesoft-inc/nebula-operator/blob/{{operator.branch}}/doc/user/nebula_cluster_helm_guide.md#optional-chart-parameters) 查看可配置的集群参数的描述及默认值。

  通过`--set`参数设置{{nebula.name}}集群的配置参数，例如，`--set nebula.storaged.replicas=3`可设置{{nebula.name}}集群中 Storage 服务的副本数为 3。


7. 查看{{nebula.name}}集群创建状态。
   
  ```bash
  kubectl -n "${NEBULA_CLUSTER_NAMESPACE}" get pod -l "app.kubernetes.io/cluster=${NEBULA_CLUSTER_NAME}"
  ```

## 扩缩容集群

{{comm.comm_begin}}
不支持扩缩容社区版的{{nebula.name}}集群。
{{comm.comm_end}}

{{ ent.ent_begin }}

仅支持通过 v1.1.0 及以上版本的 NebulaGraph Operator 扩缩容{{nebula.name}}集群。

用户可通过定义{{nebula.name}}中不同服务对应的`replicas`的值扩缩容集群。

例如，扩容{{nebula.name}}集群中 Storage 的副本数为 5（原始值为 2），命令如下：

```bash
helm upgrade "${NEBULA_CLUSTER_NAME}" nebula-operator/nebula-cluster \
    --namespace="${NEBULA_CLUSTER_NAMESPACE}" \
    --set nameOverride=${NEBULA_CLUSTER_NAME} \
    --set nebula.storageClassName="${STORAGE_CLASS_NAME}" \
    --set nebula.storaged.replicas=5
```

同理，将{{nebula.name}}集群中服务对应的`replicas`的值设置成小于原始值，即可实现集群服务的缩容。

如果缩容长时间操作处于未完成状态，可以进入通过`nebula.console`字段启动的 Console 容器，然后查看缩容 Job 状态。如果缩容 Job 处于`FAILED`状态，可以查看 Meta 服务的日志，找到导致缩容失败的原因。关于更多 Job 的信息，参见[作业管理](../../3.ngql-guide/4.job-statements.md)。

!!! caution

    - 目前仅支持对{{nebula.name}}集群中的 Graph 服务和 Storage 服务进行扩缩容，不支持扩缩容 Meta 服务。
    - 如果缩容带 Zone 功能的集群，需要确保缩容后的剩余的 Storage Pod 数量不少于通过`nebula.metad.config.zone_list`指定的 Zone 的数量。例如，如果 Zone 的数量为 3，那么缩容后的 Storage Pod 数量不能少于 3 个。

用户可点击 [nebula-cluster/values.yaml](https://github.com/vesoft-inc/nebula-operator/blob/{{operator.tag}}/charts/nebula-cluster/values.yaml) 查看 nebula-cluster chart 的更多配置。有关文件中配置项的解释，参考下文**{{nebula.name}} 集群 Chart 配置参数说明**。

{{ ent.ent_end }}

## 删除集群

使用 Helm 删除集群的命令如下：

```bash
helm uninstall "${NEBULA_CLUSTER_NAME}" --namespace="${NEBULA_CLUSTER_NAMESPACE}"
```

或者使用真实值删除集群，例如：

```bash
helm uninstall nebula --namespace=nebula
```

## 后续操作

[连接{{nebula.name}}](../4.connect-to-nebula-graph-service.md)

