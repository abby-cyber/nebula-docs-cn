# 使用 Kubectl 部署{{nebula.name}}集群

!!! Compatibility "历史版本兼容性"

    1.x 版本的 NebulaGraph Operator 不兼容 3.x 以下版本的{{nebula.name}}。

## 前提条件

- [安装 NebulaGraph Operator](../2.deploy-nebula-operator.md)
- [已创建 StorageClass](https://kubernetes.io/docs/concepts/storage/storage-classes/)
{{ent.ent_begin}}
- [已安装 LM 并加载 License Key](3.0.deploy-lm.md)
{{ent.ent_end}}

## 创建集群

本文以创建名为`nebula`的集群为例，说明如何部署{{nebula.name}}集群。

1. 创建命名空间，例如`nebula`。如果不指定命名空间，默认使用`default`命名空间。

  ```bash
  kubectl create namespace nebula
  ```

  {{ent.ent_begin}}
2. 创建 Secret，用于拉取私有仓库中{{nebula.name}}镜像。

  ```bash
  kubectl -n <nebula> create secret docker-registry <image-pull-secret> \
  --docker-server=DOCKER_REGISTRY_SERVER \
  --docker-username=DOCKER_USER \
  --docker-password=DOCKER_PASSWORD
  ```

  - `<nebula>`：存放该 Secret 的命名空间。
  - `<image-pull-secret>`：指定 Secret 的名称。
  - DOCKER_REGISTRY_SERVE：指定拉取镜像的私有仓库服务器地址，例如`reg.example-inc.com`。
  - DOCKER_USE：镜像仓库用户名。
  - DOCKER_PASSWORD：镜像仓库密码。
  {{ent.ent_end}}

3. 创建集群配置文件。
   
    {{comm.comm_begin}}
  创建名为`apps_v1alpha1_nebulacluster.yaml`的文件。文件内容参见[示例配置](https://github.com/vesoft-inc/nebula-operator/blob/v{{operator.release}}/config/samples/apps_v1alpha1_nebulacluster.yaml)。
    {{comm.comm_end}}

    {{ent.ent_begin}}
  联系销售人员获取完整配置示例。必须自定义修改以下参数，其他参数可按需修改。

  - `spec.metad.licenseManagerURL`
  - `spec.<graphd|metad|storaged>.image`
  - `spec.imagePullSecrets`
  
    {{ent.ent_end}}

  示例配置的参数描述如下：

  | 参数    | 默认值  | 描述    |
  | :---- | :--- | :--- |
  | `metadata.name`              | -                                                            | 创建的{{nebula.name}}集群名称。 |
  |`spec.console`|-| 启动 Console 容器用于连接 Graph 服务。配置详情，参见 [nebula-console](https://github.com/vesoft-inc/nebula-operator/blob/v{{operator.release}}/doc/user/nebula_console.md#nebula-console).|
  | `spec.graphd.replicas`       | `1`                                                          | Graphd 服务的副本数。         |
  | `spec.graphd.image`         | `vesoft/nebula-graphd`                                       | Graphd 服务的容器镜像。       |
  | `spec.graphd.version`        | `{{nebula.tag}}`                                                     | Graphd 服务的版本号。         |
  | `spec.graphd.service`        |                                                             | 访问 Graphd 服务的 Service 配置。      | 
  | `spec.graphd.logVolumeClaim.storageClassName`   | -                                                            | Graphd 服务的日志盘存储卷的存储类名称。使用示例配置时需要将其替换为事先创建的存储类名称，参见 [Storage Classes](https://kubernetes.io/docs/concepts/storage/storage-classes/) 查看创建存储类详情。        |
  | `spec.metad.replicas`        | `1`                                                          | Metad 服务的副本数。          |
  | `spec.metad.image`          | `vesoft/nebula-metad`                                        | Metad 服务的容器镜像。        |
  | `spec.metad.version`         | `{{nebula.tag}}`                                                     | Metad 服务的版本号。          |
  | `spec.metad.dataVolumeClaim.storageClassName`    | -                                                            | Metad 服务的数据盘存储配置。使用示例配置时需要将其替换为事先创建的存储类名称，参见 [Storage Classes](https://kubernetes.io/docs/concepts/storage/storage-classes/) 查看创建存储类详情。        |
  | `spec.metad.logVolumeClaim.storageClassName`|-|Metad 服务的日志盘存储配置。使用示例配置时需要将其替换为事先创建的存储类名称，参见 [Storage Classes](https://kubernetes.io/docs/concepts/storage/storage-classes/) 查看创建存储类详情。 |
  | `spec.storaged.replicas`     | `3`                                                          | Storaged 服务的副本数。       |
  | `spec.storaged.image`       | `vesoft/nebula-storaged`                                     | Storaged 服务的容器镜像。     |
  | `spec.storaged.version`      | `{{nebula.tag}}`                                                     | Storaged 服务的版本号。       |
  | `spec.storaged.dataVolumeClaims.resources.requests.storage` | -                                                            | Storaged 服务的数据盘存储大小，可指定多块数据盘存储数据。当指定多块数据盘时，路径为：`/usr/local/nebula/data1`、`/usr/local/nebula/data2`等。 |
  | `spec.storaged.dataVolumeClaims.resources.storageClassName` | -                                                            | Storaged 服务的数据盘存储配置。使用示例配置时需要将其替换为事先创建的存储类名称，参见 [Storage Classes](https://kubernetes.io/docs/concepts/storage/storage-classes/) 查看创建存储类详情。  |
  | `spec.storaged.logVolumeClaim.storageClassName`|-|Storaged 服务的日志盘存储配置。使用示例配置时需要将其替换为事先创建的存储类名称，参见 [Storage Classes](https://kubernetes.io/docs/concepts/storage/storage-classes/) 查看创建存储类详情。 |
  |`spec.<metad|storaged|graphd>.securityContext`|`{}`|定义集群容器的权限和访问控制，以控制访问和执行容器的操作。详情参见 [SecurityContext](https://github.com/vesoft-inc/nebula-operator/blob/{{operator.branch}}/doc/user/security_context.md)。 |
  |`spec.agent`|`{}`| Agent 服务的配置。用于备份和恢复及日志清理功能，如果不自定义该配置，将使用默认配置。|
  | `spec.reference.name`        | -                                                            | 依赖的控制器名称。           |
  | `spec.schedulerName`         | -                                                            | 调度器名称。                 |
  | `spec.imagePullPolicy`       | {{nebula.name}}镜像的拉取策略。关于拉取策略详情，请参考 [Image pull policy](https://kubernetes.io/docs/concepts/containers/images/#image-pull-policy)。 | 镜像拉取策略。               |
  |`spec.logRotate`| - |日志轮转配置。详情参见[管理集群日志](../8.custom-cluster-configurations/8.4.manage-running-logs.md)。|
  |`spec.enablePVReclaim`|`false`|定义是否在删除集群后自动删除 PVC 以释放数据。详情参见[回收 PV](../8.custom-cluster-configurations/8.2.pv-reclaim.md)。|
  
    {{ ent.ent_begin }}

  {{nebula.name}}特有参数描述如下：

  | 参数    | 默认值  | 描述    |
  | :---- | :--- | :--- |
  | `spec.metad.licenseManagerURL`       | - | 配置指向 [LM](../../9.about-license/2.license-management-suite/3.license-manager.md) 的 URL，由 LM 的访问地址和端口（默认端口`9119`）组成。例如，`192.168.8.100:9119`。**必须配置此参数以获取 License 信息，否则无法使用{{nebula.name}}集群。** |
  |`spec.storaged.enableAutoBalance`| `false`| 是否启用自动均衡。详情参见[均衡扩容后的 Storage 数据](../8.custom-cluster-configurations/8.3.balance-data-when-scaling-storage.md)。|
  |`spec.enableBR`|`false`|定义是否启用 BR 工具。详情参见[备份与恢复](../10.backup-restore-using-operator.md)。|
  |`spec.imagePullSecrets`| - |定义拉取私有仓库中镜像所需的 Secret。|

  !!! enterpriseonly

        拉取{{nebula.name}}集群镜像前，请确保已联系销售人员（[inqury@vesoft.com](mailto:inqury@vesoft.com)）获取{{nebula.name}}集群的镜像信息。

    {{ ent.ent_end }}

4. 创建{{nebula.name}}集群。

  ```bash
  kubectl create -f apps_v1alpha1_nebulacluster.yaml
  ```

  返回：

  ```bash
  nebulacluster.apps.nebula-graph.io/nebula created
  ```


5. 查看{{nebula.name}}集群状态。
   
  ```bash
  kubectl get nebulaclusters nebula
  ```

  返回：

  ```bash
  NAME     GRAPHD-DESIRED   GRAPHD-READY   METAD-DESIRED   METAD-READY   STORAGED-DESIRED   STORAGED-READY   AGE
  nebula   1                1              1               1             3                  3                86s
  ```

{{ent.ent_begin}}
## 创建带 Zone 的集群

NebulaGraph 利用 Zone 的功能来高效管理其分布式架构。每个 Zone 代表了存储完整图空间数据的 Storage Pod 的逻辑分组。NebulaGraph 的图空间数据被切分为不同分片，并且这些分片的副本均匀分布在所有可用的 Zone 中，同时查询可以优先发送到同一 Zone 内的存储 Pod 中。使用 Zone 可显着降低区域间的网络流量成本并提高数据传输速度。关于 Zone 的详细信息，请参见[管理 Zone](../../4.deployment-and-installation/5.zone.md)。

### 配置 Zone

如要充分利用 Zone 功能，首先需要确定集群节点所在的实际 Zone。通常情况下，部署在云平台上的节点都带有各自 Zone 的标签。一旦获取了这些信息，可以通过在集群的配置文件中设置`spec.metad.config.zone_list`参数来进行配置。该参数是一个由逗号分隔的 Zone 名称列表，并且应与节点实际所在的 Zone 名称相匹配。例如，如果用户节点位于 az1、az2 和 az3 区域，配置应如下所示：

```yaml
spec:
  metad:
    config:
      zone_list: az1, az2, az3
```

### 使用 Zone

NebulaGraph Operator 利用 Kubernetes 的 TopologySpread 功能来管理 Storage Pod 和 Graph Pod 的调度。一旦配置了`zone_list`，Storage Pod 将根据`topology.kubernetes.io/zone`标签自动分配到各自的 Zone 中。

对于 Zone 内数据访问，Graph Pod 会使用`--assigned_zone=$NODE_ZONE`参数动态分配自己到一个 Zone 内。通过一个 init-container 来获取节点上的 Graph Pod 所在 Zone 的名称。在`spec.alpineImage`（默认值：`reg.vesoft-inc.com/nebula-alpine:latest`）中指定的 Alpine Linux 镜像可获取 Zone 信息。

### 优先访问 Zone 内数据

通过在集群配置文件中将`spec.graphd.config.prioritize_intra_zone_reading`设置为`true`，可以使 Graph Pod 优先将查询发送到同一 Zone 内的 Storage Pod。在该 Zone 内读取失败的情况下，行为取决于`spec.graphd.config.stick_to_intra_zone_on_failure`的值。如果设置为`true`，Graph 服务将避免从其他 Zone 内读取数据并返回错误。否则，它将读取其他 Zone 内分片的 Leader 副本数据。

```yaml
spec:
  alpineImage: reg.vesoft-inc.com/xxx/xxx:latest
  graphd:
    config:
      prioritize_intra_zone_reading: "true"
      stick_to_intra_zone_on_failure: "false"
```

### 部分配置示例

如果需要创建带 Zone 的集群，必需在集群配置文件中添加以下字段。其他可选字段描述见上文**创建集群**。完整配置示例请联系销售人员获取。

```yaml
spec:
  # 用来获取节点所在的 Zone 信息。
  alpineImage: "reg.vesoft-inc.com/xxx/xxx:latest"
  graphd:
    image: reg.vesoft-inc.com/xxx/xxx
    config:
      # 优先将查询发送到同一 Zone 内的存储节点。
      prioritize_intra_zone_reading: "true"
      stick_to_intra_zone_on_failure: "false" 
  metad:
    image: reg.vesoft-inc.com/xxx/xxx
    config:
      # Zone 的名称列表，由逗号分隔。建议设置为奇数。
      zone_list: az1,az2,az3 
    licenseManagerURL: "192.168.8.xxx:9119"
  storaged:
    image: reg.vesoft-inc.com/xxx/xxx
  imagePullSecrets:
  - name: nebula-image
  # 用于控制存储 Pod 分布的字段。
  topologySpreadConstraints:
  - topologyKey: "topology.kubernetes.io/zone"
    whenUnsatisfiable: "DoNotSchedule"
```

部分参数描述如下。

| 参数                             | 默认值                 | 描述                                                                                                       |
| ------------------------------ | -------------------- | -------------------------------------------------------------------------------------------------------- |
| `spec.metad.licenseManagerURL` | -                    | 配置指向许可证管理器（LM）的 URL，其中包括 LM 的访问地址和端口号（默认端口 `9119`）。例如，`192.168.8.xxx:9119`。**必须配置此参数以获取许可证信息；否则，无法使用企业版集群。** |
| `spec.<graphd|metad|storaged>.image`      | -                    | 企业版 Graph、Meta 或 Storage 服务的容器镜像。                                                                         |
| `spec.imagePullSecrets`        | -                    | 用于从私有仓库拉取 NebulaGraph 企业版服务镜像的 Secret。                                                                     |
| `spec.alpineImage`             | -                    | 用于获取节点所在区域信息的 Alpine Linux 镜像。                                                                         |
| `spec.metad.config.zone_list`  | -                    | Zone 的名称列表，由逗号分隔。建议设置奇数个 Zone。例如：zone1,zone2,zone3。<br/>**一旦设置，区域名称无法更改。**                             |
| `spec.graphd.config.prioritize_intra_zone_reading` | `false` | 指定是否优先将查询发送到同一 Zone 内的存储节点。<br/>当设置为`true`时，查询将发送到同一 Zone 的存储节点。如果在该区 Zone 读取失败，它将根据`stick_to_intra_zone_on_failure`决定是否读数据分片的 Leader 副本数据。          |
| `spec.graphd.config.stick_to_intra_zone_on_failure` | `false` | 当设置为`false`时，如果在同一 Zone 内没有找到所请求数据时，将读取分片的 Leader 副本数据。当设置为`true`时，如果在该 Zone 内未读取到分片数据，将返回报错。|
| `spec.topologySpreadConstraints`              | -                    | 这是 Kubernetes 中用于控制存储 Pod 分布的字段。其目的是确保存储 Pod 均匀分布在各个 Zone 中。<br/>**要使用 Zone 功能，必须将`topologySpreadConstraints[0].topologyKey`的值设置为`topology.kubernetes.io/zone`，并将`topologySpreadConstraints[0].whenUnsatisfiable`的值设置为`DoNotSchedule`**。运行`kubectl get node --show-labels`来检查该键信息。有关更多信息，请参阅[TopologySpread](https://kubernetes.io/docs/concepts/scheduling-eviction/topology-spread-constraints/#example-multiple-topologyspreadconstraints)。 |

!!! warning

    请不要手动修改 NebulaGraph Operator 创建的 configmaps。这样做可能会导致意外的行为。
    
    在 Storage Pod 和 Graph Pod 被分配到了各个 Zone 后，与其对应的 Zone 之间的映射信息将存储在名为`<cluster_name>-graphd|storaged-zone`的 configmap 中。这个映射在滚动更新和 Pod 重启期间有助于 Pod 的调度，确保服务返回到所需的原始 Zone 中。

!!! caution

    创建集群后，导入数据前，确保存储 Pod 均匀分布在各个 Zone 中以提高集群的韧性。运行`SHOW ZONES`命令来检查存储 Pod 是否均匀分布在各个 Zone 中。有关 Zone 相关命令，请参阅[管理 Zone](../../4.deployment-and-installation/5.zone.md)。

{{ent.ent_end}}

## 扩缩容集群

{{comm.comm_begin}}
不支持扩缩容社区版的{{nebula.name}}集群。
{{comm.comm_end}}

{{ ent.ent_begin }}

仅支持通过 v1.1.0 及以上版本的 NebulaGraph Operator 扩缩容{{nebula.name}}集群。
  
用户可以通过编辑`apps_v1alpha1_nebulacluster.yaml`文件中的`replicas`的值进行{{nebula.name}}集群的扩缩容。

### 扩容集群

本文举例扩容{{nebula.name}}集群中 Storage 服务至 5 个。步骤如下：

1. 将`apps_v1alpha1_nebulacluster.yaml`文件中`storaged.replicas`的参数值从`3`改为`5`。

  ```yaml
    storaged:
      resources:
        requests:
          cpu: "500m"
          memory: "500Mi"
        limits:
          cpu: "1"
          memory: "1Gi"
      replicas: 5
      image: vesoft/nebula-storaged
      version: {{nebula.tag}}
      dataVolumeClaims:
      - resources:
          requests:
            storage: 2Gi
        storageClassName: fast-disks
      - resources:
          requests:
            storage: 2Gi
        storageClassName: fast-disks
      logVolumeClaim:
        resources:
          requests:
            storage: 2Gi
        storageClassName: fast-disks
    reference:
      name: statefulsets.apps
      version: v1
    schedulerName: default-scheduler
  ```

2. 执行以下命令使上述更新同步至{{nebula.name}}集群 CR 中。

  ```bash
  kubectl apply -f apps_v1alpha1_nebulacluster.yaml
  ```
  
3. 查看 Storage 服务的副本数。

  ```bash
  kubectl get pods -l app.kubernetes.io/cluster=nebula
  ```
  返回：

  ```bash
  NAME                READY   STATUS    RESTARTS   AGE
  nebula-graphd-0     1/1     Running   0          2m
  nebula-metad-0      1/1     Running   0          2m
  nebula-storaged-0   1/1     Running   0          2m
  nebula-storaged-1   1/1     Running   0          2m
  nebula-storaged-2   1/1     Running   0          2m
  nebula-storaged-3   1/1     Running   0          5m
  nebula-storaged-4   1/1     Running   0          5m
  ```
  由上可看出 Storage 服务的副本数被扩容至 5 个。

### 缩容集群

缩容集群的原理和扩容一样，用户只需将`apps_v1alpha1_nebulacluster.yaml`文件中的`replicas`的值缩小。具体操作，请参考上文的**扩容集群**部分。

如果缩容长时间操作处于未完成状态，可以进入通过`spec.console`字段启动的 Console 容器，然后查看缩容 Job 状态。如果缩容 Job 处于`FAILED`状态，可以查看 Meta 服务的日志，找到导致缩容失败的原因。关于更多 Job 的信息，参见[作业管理](../../3.ngql-guide/4.job-statements.md)。

!!! caution

    - 目前仅支持对{{nebula.name}}集群中的 Graph 服务和 Storage 服务进行扩缩容，不支持扩缩容 Meta 服务。
    - 如果缩容带 Zone 功能的集群，需要确保缩容后的剩余的 Storage Pod 数量不少于通过`spec.metad.config.zone_list`指定的 Zone 的数量。例如，如果 Zone 的数量为 3，那么缩容后的 Storage Pod 数量不能少于 3 个。

{{ ent.ent_end }}

## 删除集群

使用 Kubectl 删除{{nebula.name}}集群的命令如下：

```bash
kubectl delete -f apps_v1alpha1_nebulacluster.yaml
```

## 后续操作

[连接{{nebula.name}}数据库](../4.connect-to-nebula-graph-service.md)

